# Copyright (c) Facebook, Inc. and its affiliates.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
#
import os
import argparse
import pickle
import torch
import json
import sys
import io
import random
import time
import numpy as np

from multiprocessing.pool import ThreadPool

from tqdm import tqdm, trange
from collections import OrderedDict

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset

#from pytorch_transformers.file_utils import PYTORCH_PRETRAINED_BERT_CACHE
from pytorch_transformers.optimization import WarmupLinearSchedule
#from pytorch_transformers.tokenization_bert import BertTokenizer
from pytorch_transformers.modeling_utils import WEIGHTS_NAME

import blink.candidate_retrieval.utils
from blink.crossencoder.crossencoder import CrossEncoderRanker, load_crossencoder
import logging

import blink.candidate_ranking.utils as utils
#import blink.biencoder.data_process as data
from blink.biencoder.zeshel_utils import DOC_PATH, WORLDS, world_to_id
from blink.common.optimizer import get_bert_optimizer
from blink.common.params import BlinkParser

from blink.out_of_KB_utils import infer_out_KB_ent_cross_enc, infer_out_KB_ent_cross_enc_classify

logger = None


def modify(context_input, candidate_input, max_seq_length):
    print('max_seq_length:',max_seq_length)
    new_input = []
    context_input = context_input.tolist()
    candidate_input = candidate_input.tolist()

    # pair each context input with each cand input in the batch
    for i in range(len(context_input)):
        cur_input = context_input[i]
        cur_candidate = candidate_input[i]
        mod_input = []
        for j in range(len(cur_candidate)):
            # remove [CLS] token from candidate - so the length also minus 1
            sample = cur_input + cur_candidate[j][1:] 
            sample = sample[:max_seq_length]
            mod_input.append(sample)

        new_input.append(mod_input)

    return torch.LongTensor(new_input)

# evaluate with inference of NIL mentions (if with_NIL_infer is true)
def evaluate(reranker, eval_dataloader, device, logger, context_length, zeshel=False, silent=True,with_NIL_infer=False,nns=[], NIL_ent_id=88150, th_NIL_cross_enc=0.1,use_original_classification=True,use_NIL_classification=False, use_NIL_classification_infer=False,lambda_NIL=0.25,use_score_features=True,use_score_pooling=False,use_men_only_score_ft=False,use_extra_features=False): # nns_filtered is the nns of mentions which have correct candidates generated by the bi-encoder  

    #assert len(list(eval_dataloader)) == len(nns)
    # 'device' sets whether gpu or cpu is to be used
    reranker.model.eval() # set to eval mode
    if silent:
        iter_ = eval_dataloader
    else:
        iter_ = tqdm(eval_dataloader, desc="Evaluation")
    results = {}

    eval_accuracy = 0.0
    eval_tp_in_KB = 0.0
    eval_tp_NIL = 0.0
    nb_eval_examples = 0
    nb_eval_examples_in_KB = 0
    nb_eval_examples_NIL = 0
    nb_pred_examples_in_KB = 0
    nb_pred_examples_NIL = 0
    nb_eval_steps = 0

    acc = {}
    tot = {}
    world_size = len(WORLDS)
    for i in range(world_size):
        acc[i] = 0.0
        tot[i] = 0.0

    all_logits = []
    cnt = 0
    for step, batch in enumerate(iter_):
        if zeshel:
            src = batch[2] # batch[2] seems now occupied by tensor_is_NIL_labels, this one to be fixed later.
            cnt += 1
        batch = tuple(t.to(device) for t in batch)
        context_input = batch[0]
        data_size_batch = context_input.size(0)
        label_input = batch[1] # now my labels contain value -1 or 100 - issue to fix: /pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed. # it feels like that I am operating a large machine - BLINK is pretty complex. - solved by noy calculating loss in reranker() below
        if len(batch)>2: #TODO: fix this with zeshel case
            tensor_is_NIL_labels = batch[2]            
        else:
            tensor_is_NIL_labels = None
            # for training - this is not yet included in the batch, so set as None, but we can inlcude this in the batch
        if len(batch)>3: #TODO: fix this with zeshel case
            mention_matchable_fts = batch[3]
        else:
            mention_matchable_fts = None # this is so far for the inference stage, see main_dense._process_cross_encoder_dataloader. TODO
        #print('label input in batch %d' % step, label_input)
        #print('tensor_is_NIL_labels in batch %d:' % step, tensor_is_NIL_labels)
        # here it gets the predictions
        with torch.no_grad():
            logits, logits_NIL = reranker(context_input, label_input, context_length, inference_only=True,label_is_NIL_input=tensor_is_NIL_labels,use_original_classification=use_original_classification,use_NIL_classification=use_NIL_classification,lambda_NIL=lambda_NIL,use_score_features=use_score_features,use_score_pooling=use_score_pooling,use_men_only_score_ft=use_men_only_score_ft,use_extra_features=use_extra_features,mention_matchable_fts=mention_matchable_fts) # setting inference_only as True to avoid calculating loss 
            # this finally calls CrossEncoderRanker.forward() in crossencoder.py
        
        #if not silent:
        #   print('logits:',logits.size())
        logits = logits.detach().cpu().numpy()
        if logits_NIL is not None:
            logits_NIL = logits_NIL.detach().cpu().numpy()
        label_ids = label_input.cpu().numpy() 
        is_NIL_labels = tensor_is_NIL_labels.cpu().numpy() if not tensor_is_NIL_labels is None else None
        #print('is_NIL_labels:',is_NIL_labels)
        # you don't know what the label for the label_id is, as here it only encodes where it is appeared in the nn predictions from the biencoder. so you need tensor_is_NIL_labels.
        
        ind_out = np.argmax(logits, axis=1) # get the predicted index of the output 
        #print('ind_out:',ind_out) # an array of the predicted indexes (each is an index of the candidates from the bi-encoder)
        
        # get the batch of nns predictions
        #print('nns:',len(nns))
        nns_batch = nns[step*data_size_batch:(step+1)*data_size_batch]

        # infer out-of-KB / NIL labels from the logits using a threshold approach, if chosen to
        # and update the predicted index, ind_out, of the cross-encoder
        if with_NIL_infer:
            #print('th_NIL_cross_enc:',th_NIL_cross_enc)
            ind_out,np_arr_bool_changed_to_NIL = infer_out_KB_ent_cross_enc(logits, ind_out, nns_batch, NIL_ent_id, th_NIL_cross_enc=th_NIL_cross_enc)

        # infer out-of-KB / NIL lables based on NIL classification, if chosen to:
        if use_NIL_classification_infer:
            print('using use_NIL_classification_infer')
            ind_out,np_arr_bool_changed_to_NIL = infer_out_KB_ent_cross_enc_classify(logits_NIL,ind_out,nns_batch,NIL_ent_id)

        #print('ind_out:',ind_out) # an array of the predicted indexes (each is an index of the candidates
        #assert np.all(ind_out <= 99)
        #print('label_ids in train_cross.evaluate():',label_ids)
        #tmp_eval_accuracy, eval_result = utils.accuracy(logits, label_ids)
        tmp_eval_accuracy, eval_result = utils.accuracy_from_ind(ind_out, label_ids)
        # the first argument output, tmp_eval_accuracy, is the num of accurate instances in the batch
        tmp_eval_accuracy_in_KB, eval_result_in_KB = utils.accuracy_from_ind_is_in_KB(ind_out, label_ids,is_NIL_labels)
        tmp_eval_accuracy_NIL, eval_result_NIL = utils.accuracy_from_ind_is_NIL(ind_out, label_ids,is_NIL_labels)
        #print('tmp_eval_accuracy:',tmp_eval_accuracy)
        #print('eval_result:',eval_result)
        #print('tmp_eval_accuracy_in_KB:',tmp_eval_accuracy_in_KB, eval_result_in_KB)
        #print('tmp_eval_accuracy_NIL:',tmp_eval_accuracy_NIL, eval_result_NIL)

        eval_accuracy += tmp_eval_accuracy
        eval_tp_in_KB += tmp_eval_accuracy_in_KB
        eval_tp_NIL += tmp_eval_accuracy_NIL
        all_logits.extend(logits)

        # get the number of eval and pred examples in each category of all, in-KB, NIL/out-of-KB.
        nb_eval_examples += data_size_batch
        if not is_NIL_labels is None:
            assert data_size_batch == len(is_NIL_labels)
            nb_eval_examples_in_KB += len(is_NIL_labels) - np.count_nonzero(is_NIL_labels)
            nb_eval_examples_NIL += np.count_nonzero(is_NIL_labels)
        if with_NIL_infer or use_NIL_classification_infer:
            assert len(is_NIL_labels) == len(np_arr_bool_changed_to_NIL)
            nb_pred_examples_in_KB += len(np_arr_bool_changed_to_NIL) - np.count_nonzero(np_arr_bool_changed_to_NIL)
            nb_pred_examples_NIL += np.count_nonzero(np_arr_bool_changed_to_NIL)
        else:
            # for the case that NIL is directly predicted (for it has been represented in the entity catalogue), without threshold-based with_NIL_infer
            nb_pred_examples_NIL_in_batch = 0
            for ind, nn in enumerate(nns_batch):
                ind_NIL_ent_2d = np.where(nn==NIL_ent_id)
                ##('ind_NIL_ent_2d:',ind_NIL_ent_2d)
                if ind_NIL_ent_2d[0].size > 0:
                    # get NIL's id in the 100 candidates generated from bi-encoder if it is there
                    NIL_ent_id_in_cross = ind_NIL_ent_2d[0][0]
                    #print('NIL ind:',NIL_ent_id_in_cross)
                    #print('pred ind:',ind_out[ind])
                    if NIL_ent_id_in_cross == ind_out[ind]:
                        nb_pred_examples_NIL_in_batch += 1
            nb_pred_examples_NIL += nb_pred_examples_NIL_in_batch            
            nb_pred_examples_in_KB += (data_size_batch - nb_pred_examples_NIL_in_batch)

        if zeshel:
            for i in range(data_size_batch):
                src_w = src[i].item()
                acc[src_w] += eval_result[i]
                tot[src_w] += 1
        nb_eval_steps += 1
        
    normalized_eval_accuracy = -1
    eval_precision_in_KB = -1
    eval_precision_NIL = -1
    eval_recall_in_KB = -1
    eval_recall_NIL = -1
    #print('nb_eval_examples:',nb_eval_examples)
    if nb_eval_examples > 0:
        normalized_eval_accuracy = eval_accuracy / nb_eval_examples # so this is actually recall. but what is precision? for all labels (in-KB + out-of-KB/NIL), there is no differences in recall or precision, as the model predicts from all data and will predict it to be a
    # below the four metrics computes the precision/recall of linking a mention as in-KB/NIL
    if nb_eval_examples_in_KB > 0:
        eval_recall_in_KB = eval_tp_in_KB / nb_eval_examples_in_KB
        #print('in-KB-recall:', eval_recall_in_KB, '=', eval_tp_in_KB, '/', nb_eval_examples_in_KB)
    if nb_eval_examples_NIL > 0:
        eval_recall_NIL = eval_tp_NIL / nb_eval_examples_NIL

    #print('nb_pred_examples_in_KB:',nb_pred_examples_in_KB)
    #print('nb_pred_examples_NIL:',nb_pred_examples_NIL)
    if nb_pred_examples_in_KB > 0:
        eval_precision_in_KB = eval_tp_in_KB / nb_pred_examples_in_KB # so this is actually precision. The in-KB precision is a more strict form: not only it should be predicted as in-KB, but also should be linked correctly.
    if nb_pred_examples_NIL > 0:
        eval_precision_NIL = eval_tp_NIL / nb_pred_examples_NIL # so this is actually precision.
    
    if zeshel:
        macro = 0.0
        num = 0.0 
        for i in range(len(WORLDS)):
            if acc[i] > 0:
                acc[i] /= tot[i]
                macro += acc[i]
                num += 1
        if num > 0:
            logger.info("Macro accuracy: %.5f" % (macro / num))
            logger.info("Micro accuracy: %.5f" % normalized_eval_accuracy)
    else:
        if logger:
            logger.info("Eval accuracy: %.5f" % normalized_eval_accuracy)

    results["tp_all"] = eval_accuracy
    results["tp_in_KB"] = eval_tp_in_KB
    results["tp_NIL"] = eval_tp_NIL    
    results["precision_in_KB"] = eval_precision_in_KB
    results["precision_NIL"] = eval_precision_NIL
    results["recall_in_KB"] = eval_recall_in_KB
    results["recall_NIL"] = eval_recall_NIL
    results["f1_in_KB"] = utils.f1_valid(results["precision_in_KB"],results["recall_in_KB"])
    results["f1_NIL"] = utils.f1_valid(results["precision_NIL"],results["recall_NIL"])
    #print("tp_all:",eval_accuracy,'tp_in_KB:',eval_tp_in_KB,'tp_NIL:',eval_tp_NIL)
    #print("eval_prec_rec_f1_in_KB:",eval_precision_in_KB,eval_recall_in_KB,results["f1_in_KB"])
    #print("eval_prec_rec_f1_NIL:",eval_precision_NIL,eval_recall_NIL,results["f1_NIL"])
    # print('eval_precision_in_KB:',eval_precision_in_KB)
    # print('eval_recall_in_KB:',eval_recall_in_KB)
    # print('eval_precision_NIL:',eval_precision_NIL)
    # print('eval_recall_NIL:',eval_recall_NIL)
    results["normalized_accuracy"] = normalized_eval_accuracy # actually this is recall - for the "all labels" setting only.
    # print('normalized_eval_accuracy in train_cross.evaluate():',normalized_eval_accuracy)
    results["nb_eval_examples"] = nb_eval_examples
    results["nb_eval_examples_in_KB"] = nb_eval_examples_in_KB
    results["nb_eval_examples_NIL"] = nb_eval_examples_NIL
    #print("nb_eval_all:",nb_eval_examples,"in_KB:",nb_eval_examples_in_KB,"NIL:",nb_eval_examples_NIL)
    results["logits"] = all_logits

    if logger:
        #logger.info("Eval accuracy: %.5f" % normalized_eval_accuracy)
        logger.info('nb_pred_examples_in_KB: %d' % nb_pred_examples_in_KB)
        logger.info('nb_pred_examples_NIL: %d' % nb_pred_examples_NIL)
        logger.info("tp_all: %d tp_in_KB: %d tp_NIL: %d" % (eval_accuracy,eval_tp_in_KB,eval_tp_NIL))
        logger.info("eval_prec_rec_f1_in_KB: %.5f %.5f %.5f" % (eval_precision_in_KB,eval_recall_in_KB,results["f1_in_KB"]))
        logger.info("eval_prec_rec_f1_NIL: %.5f %.5f %.5f" % (eval_precision_NIL,eval_recall_NIL,results["f1_NIL"]))
        logger.info("nb_eval_all: %d in_KB: %d NIL: %d" % (nb_eval_examples,nb_eval_examples_in_KB,nb_eval_examples_NIL))
    return results

# evaluate the predicted edges for insertion
def evaluate_edges(reranker, eval_dataloader, device, logger, context_length, zeshel=False, silent=True): # nns_filtered is the nns of mentions which have correct candidates generated by the bi-encoder  

    #assert len(list(eval_dataloader)) == len(nns)
    # 'device' sets whether gpu or cpu is to be used
    reranker.model.eval() # set to eval mode
    if silent:
        iter_ = eval_dataloader
    else:
        iter_ = tqdm(eval_dataloader, desc="Evaluation")
    results = {}

    eval_accuracy = 0.0
    nb_eval_examples = 0
    nb_eval_steps = 0

    acc = {}
    tot = {}
    world_size = len(WORLDS)
    for i in range(world_size):
        acc[i] = 0.0
        tot[i] = 0.0

    all_logits = []
    cnt = 0
    for step, batch in enumerate(iter_):
        if zeshel:
            src = batch[2] # batch[2] seems now occupied by tensor_is_NIL_labels, this one to be fixed later.
            cnt += 1
        batch = tuple(t.to(device) for t in batch)
        context_input = batch[0]
        data_size_batch = context_input.size(0)
        label_input = batch[1] # now my labels contain value -1 or 100 - issue to fix: /pytorch/aten/src/THCUNN/ClassNLLCriterion.cu:108: cunn_ClassNLLCriterion_updateOutput_kernel: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed. # it feels like that I am operating a large machine - BLINK is pretty complex. - solved by noy calculating loss in reranker() below
        if len(batch)>2: #TODO: fix this with zeshel case
            tensor_is_NIL_labels = batch[2]            
        else:
            tensor_is_NIL_labels = None
            # for training - this is not yet included in the batch, so set as None, but we can inlcude this in the batch
        if len(batch)>3: #TODO: fix this with zeshel case
            mention_matchable_fts = batch[3]
        else:
            mention_matchable_fts = None # this is so far for the inference stage, see main_dense._process_cross_encoder_dataloader. TODO
        #print('label input in batch %d' % step, label_input)
        #print('tensor_is_NIL_labels in batch %d:' % step, tensor_is_NIL_labels)
        # here it gets the predictions
        with torch.no_grad():
            logits = reranker(context_input, label_input, context_length, inference_only=True,label_is_NIL_input=tensor_is_NIL_labels) # setting inference_only as True to avoid calculating loss 
            # this finally calls CrossEncoderRanker.forward() in crossencoder.py
        
        #if not silent:
        #   print('logits:',logits.size())
        logits = logits.detach().cpu().numpy()
        label_ids = label_input.cpu().numpy() 
        is_NIL_labels = tensor_is_NIL_labels.cpu().numpy() if not tensor_is_NIL_labels is None else None
        #print('is_NIL_labels:',is_NIL_labels)
        # you don't know what the label for the label_id is, as here it only encodes where it is appeared in the nn predictions from the biencoder. so you need tensor_is_NIL_labels.
        
        ind_out = np.argmax(logits, axis=1) # get the predicted index of the output 
        #print('ind_out:',ind_out) # an array of the predicted indexes (each is an index of the candidates from the bi-encoder)
        
        # get the batch of nns predictions
        #print('nns:',len(nns))
        #nns_batch = nns[step*data_size_batch:(step+1)*data_size_batch]

        #print('ind_out:',ind_out) # an array of the predicted indexes (each is an index of the candidates
        #assert np.all(ind_out <= 99)
        #print('label_ids in train_cross.evaluate():',label_ids)
        #tmp_eval_accuracy, eval_result = utils.accuracy(logits, label_ids)
        tmp_eval_accuracy, eval_result = utils.accuracy_from_ind(ind_out, label_ids)
        # the first argument output, tmp_eval_accuracy, is the num of accurate instances in the batch
        #print('tmp_eval_accuracy:',tmp_eval_accuracy)
        #print('eval_result:',eval_result)
        #print('tmp_eval_accuracy_in_KB:',tmp_eval_accuracy_in_KB, eval_result_in_KB)
        #print('tmp_eval_accuracy_NIL:',tmp_eval_accuracy_NIL, eval_result_NIL)

        eval_accuracy += tmp_eval_accuracy
        all_logits.extend(logits)

        # get the number of eval and pred examples in each category of all, in-KB, NIL/out-of-KB.
        nb_eval_examples += data_size_batch

        if zeshel:
            for i in range(data_size_batch):
                src_w = src[i].item()
                acc[src_w] += eval_result[i]
                tot[src_w] += 1
        nb_eval_steps += 1
        
    normalized_eval_accuracy = -1
    #print('nb_eval_examples:',nb_eval_examples)
    if nb_eval_examples > 0:
        normalized_eval_accuracy = eval_accuracy / nb_eval_examples # so this is actually recall. but what is precision? for all labels (in-KB + out-of-KB/NIL), there is no differences in recall or precision, as the model predicts from all data and will predict it to be a
    
    if zeshel:
        macro = 0.0
        num = 0.0 
        for i in range(len(WORLDS)):
            if acc[i] > 0:
                acc[i] /= tot[i]
                macro += acc[i]
                num += 1
        if num > 0:
            logger.info("Macro accuracy: %.5f" % (macro / num))
            logger.info("Micro accuracy: %.5f" % normalized_eval_accuracy)
    else:
        if logger:
            logger.info("Eval accuracy: %.5f" % normalized_eval_accuracy)

    results["tp_all"] = eval_accuracy
    #print("tp_all:",eval_accuracy,'tp_in_KB:',eval_tp_in_KB,'tp_NIL:',eval_tp_NIL)
    #print("eval_prec_rec_f1_in_KB:",eval_precision_in_KB,eval_recall_in_KB,results["f1_in_KB"])
    #print("eval_prec_rec_f1_NIL:",eval_precision_NIL,eval_recall_NIL,results["f1_NIL"])
    # print('eval_precision_in_KB:',eval_precision_in_KB)
    # print('eval_recall_in_KB:',eval_recall_in_KB)
    # print('eval_precision_NIL:',eval_precision_NIL)
    # print('eval_recall_NIL:',eval_recall_NIL)
    results["normalized_accuracy"] = normalized_eval_accuracy # actually this is recall - for the "all labels" setting only.
    # print('normalized_eval_accuracy in train_cross.evaluate():',normalized_eval_accuracy)
    results["nb_eval_examples"] = nb_eval_examples
    #print("nb_eval_all:",nb_eval_examples,"in_KB:",nb_eval_examples_in_KB,"NIL:",nb_eval_examples_NIL)
    results["logits"] = all_logits

    if logger:
        #logger.info("Eval accuracy: %.5f" % normalized_eval_accuracy)
        logger.info("tp_all: %d" % (eval_accuracy))
        logger.info("nb_eval_all: %d" % (nb_eval_examples))
    return results

def get_optimizer(model, params):
    return get_bert_optimizer(
        [model],
        params["type_optimization"],
        params["learning_rate"],
        fp16=params.get("fp16"),
    )


def get_scheduler(params, optimizer, len_train_data, logger):
    batch_size = params["train_batch_size"]
    grad_acc = params["gradient_accumulation_steps"]
    epochs = params["num_train_epochs"]

    num_train_steps = int(len_train_data / batch_size / grad_acc) * epochs
    num_warmup_steps = int(num_train_steps * params["warmup_proportion"])

    scheduler = WarmupLinearSchedule(
       optimizer, warmup_steps=num_warmup_steps, t_total=num_train_steps,
    ) # source code here https://huggingface.co/transformers/v1.2.0/_modules/pytorch_transformers/optimization.html
    ''' Linear warmup and then linear decay.
        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.
        Linearly decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps.
    ''' # so epochs have an effect on warmup_steps and WarmupLinearSchedule. 

    logger.info(" Num optimization steps = %d" % num_train_steps)
    logger.info(" Num warmup steps = %d", num_warmup_steps)
    return scheduler


def main(params):
    # Fix the random seeds (part 1)
    if params["fix_seeds"]:
        seed = params["seed"]
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
    
    model_output_path = params["output_path"]
    if not os.path.exists(model_output_path):
        os.makedirs(model_output_path)
    logger = utils.get_logger(params["output_path"])

    # display model training settings
    if params["use_ori_classification"]:
        logger.info("use_ori_classification")
    if params["use_NIL_classification"]:
        logger.info("use_NIL_classification")
        logger.info("lambda_NIL: %s", str(params["lambda_NIL"]))
        if params["use_score_features"]:
            logger.info("use_score_features")
        if params["use_score_pooling"]:
            logger.info("use_score_pooling")    
        if params["use_men_only_score_ft"]:
            logger.info("use_men_only_score_ft")
        if params["use_extra_features"]:
            logger.info("use_extra_features")   
        if params["use_NIL_classification_infer"]:
            logger.info("use_NIL_classification_infer")            
    # if params
    # logger.info('')

    # Init model - and here we also expect extra features 
    reranker = CrossEncoderRanker(params)
    tokenizer = reranker.tokenizer
    model = reranker.model

    # utils.save_model(model, tokenizer, model_output_path)

    device = reranker.device
    n_gpu = reranker.n_gpu
    if params["fix_seeds"]:
        # Fix the random seeds (part 2)
        if n_gpu > 0:
            torch.cuda.manual_seed_all(seed)

    if params["gradient_accumulation_steps"] < 1:
        raise ValueError(
            "Invalid gradient_accumulation_steps parameter: {}, should be >= 1".format(
                params["gradient_accumulation_steps"]
            )
        )

    # An effective batch size of `x`, when we are accumulating the gradient accross `y` batches will be achieved by having a batch size of `z = x / y`
    # args.gradient_accumulation_steps = args.gradient_accumulation_steps // n_gpu
    params["train_batch_size"] = (
        params["train_batch_size"] // params["gradient_accumulation_steps"]
    )
    train_batch_size = params["train_batch_size"]
    eval_batch_size = params["eval_batch_size"]
    grad_acc_steps = params["gradient_accumulation_steps"]

    max_seq_length = params["max_seq_length"]
    context_length = params["max_context_length"]
    
    fname = os.path.join(params["data_path"], "train.t7") # this file is generated with eval_biencoder.py (see https://github.com/facebookresearch/BLINK/issues/92#issuecomment-1126293605)
    train_data = torch.load(fname)
    context_input = train_data["context_vecs"]
    candidate_input = train_data["candidate_vecs"]
    label_input = train_data["labels"]
    label_is_NIL_input = train_data["labels_is_NIL"]
    #mention_matchable_fts = train_data["mention_matchable_fts"]
    #nns = train_data["entity_inds"]
    #print('nns:',len(nns),len(nns[0]))
    if params["debug"]:
        max_n = 200
        context_input = context_input[:max_n]
        print('context_input:',context_input[:10])
        candidate_input = candidate_input[:max_n]
        label_input = label_input[:max_n]
        label_is_NIL_input = label_is_NIL_input[:max_n]
        print('label_is_NIL_input:',label_is_NIL_input[:10])
        #mention_matchable_fts = mention_matchable_fts[:max_n]
        #nns = nns[:max_n]

    context_input = modify(context_input, candidate_input, max_seq_length)
    print('context_input',context_input.size()) 
    #context_input torch.Size([200, 100, 159]) debug mode
    #context_input torch.Size([5006, 100, 159])
    print('label_input:',label_input.size(),label_input) 
    #label_input: torch.Size([5006]) tensor([ 0,  3,  0,  ...,  5, 18,  0])

    if params["zeshel"]:
        src_input = train_data['worlds'][:len(context_input)]
        train_tensor_data = TensorDataset(context_input, label_input, src_input, label_is_NIL_input) #, mention_matchable_fts
    else:
        train_tensor_data = TensorDataset(context_input, label_input, label_is_NIL_input) #, mention_matchable_fts
    train_sampler = RandomSampler(train_tensor_data)

    train_dataloader = DataLoader(
        train_tensor_data, 
        sampler=train_sampler, 
        batch_size=params["train_batch_size"]
    )

    fname = os.path.join(params["data_path"], "valid.t7")
    valid_data = torch.load(fname)
    context_input = valid_data["context_vecs"]
    candidate_input = valid_data["candidate_vecs"]
    label_input = valid_data["labels"]
    label_is_NIL_input = valid_data["labels_is_NIL"]
    nns_valid = valid_data["entity_inds"]
    #mention_matchable_fts = valid_data["mention_matchable_fts"]
    #print('nns_valid:',nns_valid,len(nns_valid))#,len(nns_valid[0]))
    if params["debug"]:
        max_n = 200
        context_input = context_input[:max_n]
        candidate_input = candidate_input[:max_n]
        label_input = label_input[:max_n]
        nns_valid = nns_valid[:max_n]
        #mention_matchable_fts = mention_matchable_fts[:max_n]

    context_input = modify(context_input, candidate_input, max_seq_length)
    if params["zeshel"]:
        src_input = valid_data["worlds"][:len(context_input)]
        valid_tensor_data = TensorDataset(context_input, label_input, src_input, label_is_NIL_input)#, mention_matchable_fts)
    else:
        valid_tensor_data = TensorDataset(context_input, label_input, label_is_NIL_input)#, mention_matchable_fts)
    valid_sampler = SequentialSampler(valid_tensor_data)

    valid_dataloader = DataLoader(
        valid_tensor_data, 
        sampler=valid_sampler, 
        batch_size=params["eval_batch_size"]
    )

    # evaluate before training
    results = evaluate_edges(
        reranker,
        valid_dataloader,
        device=device,
        logger=logger,
        context_length=context_length,
        zeshel=params["zeshel"],
        silent=params["silent"],
        # nns=nns_valid, 
        # NIL_ent_id=params["NIL_ent_ind"],
        # use_original_classification=params["use_ori_classification"],
        # use_NIL_classification=params["use_NIL_classification"],
        # use_NIL_classification_infer=params["use_NIL_classification_infer"],
        # lambda_NIL=params["lambda_NIL"],
        # use_score_features=params["use_score_features"],
        # use_score_pooling=params["use_score_pooling"],
        # use_men_only_score_ft=params["use_men_only_score_ft"],
        # use_extra_features=params["use_extra_features"],
    )

    number_of_samples_per_dataset = {}

    time_start = time.time()

    utils.write_to_file(
        os.path.join(model_output_path, "training_params.txt"), str(params)
    )

    logger.info("Starting training")
    logger.info(
        "device: {} n_gpu: {}, distributed training: {}".format(device, n_gpu, False)
    )

    optimizer = get_optimizer(model, params)
    scheduler = get_scheduler(params, optimizer, len(train_tensor_data), logger)

    model.train()

    best_epoch_idx = (-1)
    best_score = -1

    num_train_epochs = params["num_train_epochs"]

    for epoch_idx in trange(int(num_train_epochs), desc="Epoch"):
        tr_loss = 0
        results = None

        if params["silent"]:
            iter_ = train_dataloader
        else:
            if params["limit_by_train_steps"]:
                iter_ = tqdm(train_dataloader, 
                         desc="Batch", 
                         total=min(len(train_dataloader),params["max_num_train_steps"]))
            else:
                iter_ = tqdm(train_dataloader, 
                         desc="Batch")
        part = 0
        for step, batch in enumerate(iter_):
            if params["limit_by_train_steps"] and step == params["max_num_train_steps"]:
                break
            batch = tuple(t.to(device) for t in batch)
            context_input = batch[0] 
            label_input = batch[1]
            label_is_NIL_input = batch[2]
            #mention_matchable_fts = batch[3]
            loss, _ = reranker(context_input, label_input, context_length, label_is_NIL_input=label_is_NIL_input,
                                #   use_original_classification=params["use_ori_classification"],
                                #   use_NIL_classification=params["use_NIL_classification"],lambda_NIL=params["lambda_NIL"],use_score_features=params["use_score_features"],
                                #   use_score_pooling=params["use_score_pooling"],
                                #   use_men_only_score_ft=params["use_men_only_score_ft"],
                                #   use_extra_features=params["use_extra_features"],mention_matchable_fts=None,
                                  )

            # if n_gpu > 1:
            #     loss = loss.mean() # mean() to average on multi-gpu.

            if grad_acc_steps > 1:
                loss = loss / grad_acc_steps

            tr_loss += loss.item()

            if (step + 1) % (params["print_interval"] * grad_acc_steps) == 0:
                logger.info(
                    "Step {} - epoch {} average loss: {}\n".format(
                        step,
                        epoch_idx,
                        tr_loss / (params["print_interval"] * grad_acc_steps),
                    )
                )
                tr_loss = 0

            loss.backward()

            if (step + 1) % grad_acc_steps == 0:
                torch.nn.utils.clip_grad_norm_(
                    model.parameters(), params["max_grad_norm"]
                )
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()

            if (step + 1) % (params["eval_interval"] * grad_acc_steps) == 0:
                logger.info("Evaluation on the development dataset")
                results = evaluate_edges(
                    reranker,
                    valid_dataloader,
                    device=device,
                    logger=logger,
                    context_length=context_length,
                    zeshel=params["zeshel"],
                    silent=params["silent"],
                    # nns=nns_valid, 
                    # NIL_ent_id=params["NIL_ent_ind"],
                    # use_original_classification=params["use_ori_classification"],
                    # use_NIL_classification=params["use_NIL_classification"],
                    # use_NIL_classification_infer=params["use_NIL_classification_infer"],
                    # lambda_NIL=params["lambda_NIL"],
                    # use_score_features=params["use_score_features"],
                    # use_score_pooling=params["use_score_pooling"],
                    # use_men_only_score_ft=params["use_men_only_score_ft"],
                    # use_extra_features=params["use_extra_features"],
                )
                if params["optimize_NIL"]:
                    ls = [best_score, results["f1_NIL"]]
                else:
                    ls = [best_score, results["normalized_accuracy"]]
                li = [best_epoch_idx, (epoch_idx, part)]

                best_score = ls[np.argmax(ls)]
                best_epoch_idx = li[np.argmax(ls)]

                if params["save_model_epoch_parts"]:
                    logger.info("***** Saving fine - tuned model *****")
                    epoch_output_folder_path = os.path.join(
                    model_output_path, "epoch_{}_{}".format(epoch_idx, part)
                    )
                    part += 1
                    utils.save_model(model, tokenizer, epoch_output_folder_path)
                    # utils.write_to_file( # also save the training parameters
                    #    os.path.join(epoch_output_folder_path, "training_params.txt"), str(params)
                    # )
                model.train()
                logger.info("\n")

        logger.info("***** Saving fine - tuned model *****")
        epoch_output_folder_path = os.path.join(
            model_output_path, "epoch_{}".format(epoch_idx)
        )
        utils.save_model(model, tokenizer, epoch_output_folder_path)
        # utils.write_to_file(
        #     os.path.join(epoch_output_folder_path, "training_params.txt"), str(params)
        # )
        # reranker.save(epoch_output_folder_path)

        # this is the evaluation after each epoch run, also on the development set.
        #output_eval_file = os.path.join(epoch_output_folder_path, "eval_results.txt")
        results = evaluate_edges(
            reranker,
            valid_dataloader,
            device=device,
            logger=logger,
            context_length=context_length,
            zeshel=params["zeshel"],
            silent=params["silent"],
            # nns=nns_valid, 
            # NIL_ent_id=params["NIL_ent_ind"],
            # use_original_classification=params["use_ori_classification"],
            # use_NIL_classification=params["use_NIL_classification"],
            # use_NIL_classification_infer=params["use_NIL_classification_infer"],
            # lambda_NIL=params["lambda_NIL"],
            # use_score_features=params["use_score_features"],
            # use_score_pooling=params["use_score_pooling"],
            # use_men_only_score_ft=params["use_men_only_score_ft"],
            # use_extra_features=params["use_extra_features"],
        )

        if params["optimize_NIL"]:
            ls = [best_score, results["f1_NIL"]]
        else:
            ls = [best_score, results["normalized_accuracy"]]
        li = [best_epoch_idx, epoch_idx]

        best_score = ls[np.argmax(ls)]
        best_epoch_idx = li[np.argmax(ls)]
        logger.info("\n")

    execution_time = (time.time() - time_start) / 60
    utils.write_to_file(
        os.path.join(model_output_path, "training_time.txt"),
        "The training took {} minutes\n".format(execution_time),
    )
    logger.info("The training took {} minutes\n".format(execution_time))

    # save the best model in the parent_dir
    if type(best_epoch_idx)==int:
        logger.info("Best performance in epoch: {}".format(best_epoch_idx))
    else:
        assert type(best_epoch_idx)==tuple and len(best_epoch_idx) == 2
        logger.info("Best performance in epoch: {}_{}".format(best_epoch_idx[0],best_epoch_idx[1]))
    params["path_to_model"] = os.path.join(
        model_output_path, 
        "epoch_{}".format(best_epoch_idx) if type(best_epoch_idx)==int else "epoch_{}_{}".format(best_epoch_idx[0],best_epoch_idx[1]),
        WEIGHTS_NAME,
    )
    reranker = load_crossencoder(params)
    utils.save_model(reranker.model, tokenizer, model_output_path)

    if params["evaluate"]:
        params["path_to_model"] = model_output_path
        evaluate_edges(params, logger=logger) #TODO: check here
        
if __name__ == "__main__":
    parser = BlinkParser(add_model_args=True)
    parser.add_training_args()
    parser.add_eval_args()

    # args = argparse.Namespace(**params)
    args = parser.parse_args()
    print(args)

    params = args.__dict__
    main(params)
